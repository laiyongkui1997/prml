#!/usr/bin/bash python3
#-*- coding: utf8 -*-

"""
algorithm introduction

domain: unconstrained optimization problem
description: the optimization of `Quasi-Newton method`. 
    There are four method to solve unconstrained optimization problem: 
        1. Gradient Descent method, 
        2. Newton method, 
        3. Quasi-Newton method, 
        4. BFGS and L-BFGS.
    
    Goal:
        Find the minimum point of function `f(x)` by updating parameter `x` step by step.
        Result is `x*`

    Gradient descent method:
        Note: All gradient descent related methods will use Taylor's series(æ³°å‹’çº§æ•°) which says 
              a function `f(x)` has `n+1` order derivative in the neighborhood of point `x0`.
              So function `f(x)` can unfold as a `n` order Taylor's series in that neighborhood as shown in follow:
                    ` f(x) = f(x0) + âˆ‡f(x0)(x-x0) + ( âˆ‡f(x0)(x-x0) )^2 / 2! + ... + ( âˆ‡f(x0)(x-x0) )^n / n! `
        
        Content: Use first order Taylor's series to deduce(æ¨å¯¼) how to update parameter `x`.
                    ` f(x) = f(xğ‘˜) + âˆ‡f(xğ‘˜)(x-xğ‘˜) ` => ` f(xğ‘˜) - f(x) = - âˆ‡f(xğ‘˜)(x-xğ‘˜) `
                 
                 The goal is to minimize `f(x)`, the same as maximize `f(xğ‘˜) - f(x)`,
                 the same as minimize `âˆ‡f(xğ‘˜)(x-xğ‘˜)`.
                 
                 Because `âˆ‡f(xğ‘˜)` is a constant for `x`, so when `x - xğ‘˜ = - âˆ‡f(xğ‘˜)`, `âˆ‡f(xğ‘˜)(x-xğ‘˜)` will be minimum.
                 
                 Then we can get the updating formula of parameter `x`: `x_{k+1} := xğ‘˜ - âˆ‡f(xğ‘˜)`.
                 
                 Most of the time, we want to control the step scale(æ­¥è·) we move, 
                 so we usually add a scale coefficient for `âˆ‡f(xğ‘˜)`, we call it `ğ›¼`.

                 The result updating formula of parameter `x` is `x_{k+1} := xğ‘˜ - ğ›¼âˆ‡f(xğ‘˜)`.
        
    Newton method:
        Note: The properties of positive definite matrices(æ­£å®šçŸ©é˜µ, PDM): 
                `A` is PDM when there is a matrix X make the formula `X A X > 0` is right.

        Content: Use second order Taylor's series to deduce how to update parameter 'x'.
                    ` f(x) = f(xğ‘˜) + âˆ‡f(xğ‘˜)(x-xğ‘˜) + 1/2 (x-xğ‘˜)^T âˆ‡f(xğ‘˜)^2 (x-xğ‘˜) `
                 
                 The goal is to minimize `f(x)`, the same as when get parameter `x` from `âˆ‡f(x) = 0`
                 `f(x)` is the minimum (Because `f(x)` is a second order function).

                 => ` âˆ‡f(xğ‘˜) +  âˆ‡f(xğ‘˜)^2 (x-xğ‘˜) = 0`
                 => ` x_{k+1} := xğ‘˜ - H^(-1) âˆ‡f(xğ‘˜) `, in which `H = âˆ‡f(xğ‘˜)^2`

                 The updating direction of parameter `x` is `- H^(-1) âˆ‡f(xğ‘˜) ` which goes in opposite direction with `âˆ‡f(xğ‘˜)`,
                 so we can derivate the formula: ` - âˆ‡f(xğ‘˜) H^(-1) âˆ‡f(xğ‘˜) < 0`, 
                 which clearly show that the Hesse Matrix (`H^(-1)`) need to be positive definite matrices(æ­£å®šçŸ©é˜µ).
                 This is the a disadvantage of `Newton method`.

                 As usually, we should control the step scale in updating direction,
                 but it is different from `Gradient descent method`, we should choose the best step scale.
                 => ğœ†k = arg min f( xğ‘˜ - ğœ†k H^(-1) âˆ‡f(xğ‘˜) )

                 The result updating formula of parameter `x` is `x_{k+1} := xğ‘˜ - ğœ†k H^(-1) âˆ‡f(xğ‘˜)`
                 where `ğœ†k = arg min f( xğ‘˜ - ğœ†k H^(-1) âˆ‡f(xğ‘˜) )`.

        Conclusion: 1. Need calculate Hesse Matrix which cost a lot computation, 
                       and limit Hesse Matrix to be Positive Definite Matrices.
                    2. Newton method is second order convergence(æ”¶æ•›), 
                       if the second order derivation `Î”(ğ‘¥ğ‘˜)` is not positive (0 or negetive),
                       then the direction of `x_{k+1}` may not be the right descent direction.
                    3. Newton method uses a quadric(äºŒæ¬¡æ›²é¢) to fit the local surface of current point,
                       while Gradient descent method use a plane(å¹³é¢) to fit,
                       which make Newton method more suitable for the real optimal descent path(æœ€ä¼˜ä¸‹é™è·¯å¾„).

    Quasi-Newton method:
        Content: It overcomes the biggest disadvantage for large computation. 
                 It uses a Symetric Positive Definite Matrix(å¯¹ç§°æ­£å®šçŸ©é˜µ) of approximate Hesse Matrix 
                 rather than calculate the Hesse Matrix of objective function.

                 As same as Newton method, we unfold `f(x)` in `x_{k+1}`:
                    ` f(x) = f(x_{k+1}) + âˆ‡f(x_{k+1})(x-x_{k+1}) + 1/2 (x-x_{k+1})^T âˆ‡f(x_{k+1})^2 (x-x_{k+1}) `
                
                 Then we derivate `x_{k+1}` in both sides: 
                    => ` âˆ‡f(x) = âˆ‡f(x_{k+1}) + âˆ‡f(x_{k+1})^2 (x-x_{k+1}) `
                    => ` âˆ‡f(x) = âˆ‡f(x_{k+1}) + H_{k+1} (x-x_{k+1}) `, `H_{k+1} = âˆ‡f(x_{k+1})^2`
                
                 Make `x = xğ‘˜`, => ` âˆ‡f(xğ‘˜) = âˆ‡f(x_{k+1}) + H_{k+1} (xğ‘˜-x_{k+1}) `
                    => ` âˆ‡f(x_{k+1}) - âˆ‡f(xğ‘˜) =  H_{k+1} (x_{k+1} - xğ‘˜) `

                 There, we can use a Symetric Positive Definite Matrix `B_{k+1}` to replace Hesse Matrix `H_{k+1}` as follow:
                    => ` âˆ‡f(x_{k+1}) - âˆ‡f(xğ‘˜) =  B_{k+1} (x_{k+1} - xğ‘˜) `
                 which is `Quasi-Newton Conditions`.

                 Then the problem is how to construct this Symetric Positive Definite Matrix(å¯¹ç§°æ­£å®šçŸ©é˜µ),
                 there are two methods:
                    1. BFGS
                    2. L-BFGS

        BFGS:
            Content: 1. Initialize B0 = I
                     2. Updating `B_{k+1} = Bğ‘˜ + Î”Bğ‘˜, k=1, 2, ...` iteratively

                     How to caculate Î”Bğ‘˜?
                        => ` Î”Bğ‘˜ = ğ›¼ğ‘¢ğ‘¢^ğ‘‡+ğ›½ğ‘£ğ‘£^ğ‘‡ `, `ğ‘¢` and `ğ‘£` is unkonwn.
                     which make sure that `Î”Bğ‘˜` is a Symetric Positive Definite Matrix(å¯¹ç§°æ­£å®šçŸ©é˜µ).

                     According `Quasi-Newton Conditions`:
                        => ` âˆ‡f(x_{k+1}) - âˆ‡f(xğ‘˜) =  B_{k+1} (x_{k+1} - xğ‘˜) `
                                                 = ` (Bğ‘˜ + Î”Bğ‘˜)(x_{k+1} - xğ‘˜) `
                                                 = ` (Bğ‘˜ + ğ›¼ğ‘¢ğ‘¢^ğ‘‡+ğ›½ğ‘£ğ‘£^ğ‘‡)(x_{k+1} - xğ‘˜) `
                                                 = ` Bğ‘˜Â·(x_{k+1} - xğ‘˜) + ( ğ›¼ğ‘¢^ğ‘‡Â·(x_{k+1} - xğ‘˜) )Â·ğ‘¢ + ( ğ›½ğ‘£^ğ‘‡Â·(x_{k+1} - xğ‘˜) )Â·ğ‘£ `
                        To simplity the formula, we define 
                            `Î”xğ‘˜ = (x_{k+1} - xğ‘˜)` and 
                            `Î”fğ‘˜ = âˆ‡f(x_{k+1}) - âˆ‡f(xğ‘˜)`, 
                        then the above formula can be present as follow:
                            => ` Î”fğ‘˜ =  Bğ‘˜Â·Î”xğ‘˜ + ( ğ›¼ğ‘¢^ğ‘‡Â·Î”xğ‘˜ )Â·ğ‘¢ + ( ğ›½ğ‘£^ğ‘‡Â·Î”xğ‘˜ )Â·ğ‘£ `
                        where `ğ›¼ğ‘¢^ğ‘‡Â·Î”xğ‘˜` and `ğ›½ğ‘£^ğ‘‡Â·Î”xğ‘˜` is real.
                    
                    So we can make equation as follow:
                        => `ğ›¼ğ‘¢^ğ‘‡Â·Î”xğ‘˜ = 1`
                        => `ğ›½ğ‘£^ğ‘‡Â·Î”xğ‘˜ = -1`
                    Then `ğ‘¢ - ğ‘£ = Î”fğ‘˜ - Bğ‘˜Â·Î”xğ‘˜`,
                    where we can get a approximate result:
                        => `ğ‘¢ = Î”fğ‘˜`
                        => `ğ‘£ = Bğ‘˜Â·Î”xğ‘˜`
                    
                    ----- (Get the result `ğ‘¢` and `ğ‘¢`) -----

                        => `ğ›¼ = 1 / (ğ‘¢^ğ‘‡Â·Î”xğ‘˜) = 1 / ( (Î”fğ‘˜)^ğ‘‡ Â· Î”xğ‘˜ )`
                        => `ğ›½ = -1 / (ğ‘£^ğ‘‡Â·Î”xğ‘˜) = 1 / ( (Bğ‘˜Â·Î”xğ‘˜)^ğ‘‡ Â· Î”xğ‘˜ ) = 1 / ( (Î”xğ‘˜)^ğ‘‡ Â· Bğ‘˜ Â· Î”xğ‘˜ )`
                            where `Bğ‘˜` is a Symetric Positive Definite Matrix(å¯¹ç§°æ­£å®šçŸ©é˜µ)

                    ----- (Get the result `ğ›¼` and `ğ›¼`) -----

                        => `Î”Bğ‘˜ = ğ›¼ğ‘¢ğ‘¢^ğ‘‡ + ğ›½ğ‘£ğ‘£^ğ‘‡ `
                                = ` (Î”fğ‘˜ Â· (Î”fğ‘˜)^T ) / ( (Î”fğ‘˜)^ğ‘‡ Â· Î”xğ‘˜ ) `
                                    + ` (Bğ‘˜ Â· Î”xğ‘˜ Â· (Î”xğ‘˜)^T Â· Bğ‘˜) / ( (Î”xğ‘˜)^ğ‘‡ Â· Bğ‘˜ Â· Î”xğ‘˜ )`
                    
                    ----- (Get the result `Î”Bğ‘˜`) -----

            Problem: The direction of Newton method is `- Hğ‘˜^(-1) âˆ‡f(xğ‘˜)`, so we should caculate `Bğ‘˜^(-1)`.
                     The best solution is caculating `Bğ‘˜^(-1)` directly rather than caculating `Bğ‘˜`.

    for more information, please see: 
        http://note.youdao.com/noteshare?id=2483fa0c28f55d3ea00e538fd4549e70

"""

import os
import numpy as np
import matplotlib.pyplot as plt


def draw_result(x, y):
    fig, ax = plt.subplots()
    ax.plot(x, y)
    ax.set(xlabel='x', ylabel='y', title='test function')
    ax.grid()
    plt.show()


def BFGS(fn, delta_fn, stop_fn=None, x_0=0, max_steps=50):
    results = []
    I = np.eye(x_0.shape[0])
    D = I
    b = 0.55
    p = 0.4
    sigma = 0.6
    for _ in range(max_steps):
        d = - np.dot(D, delta_fn(x_0))  # direction = - H * f'
        m = 0
        while m < 20:
            a = b**m
            # half Armijo condition (in this test, can reach the optimal point)
            # if fn(x_0 + a * d) <= fn(x_0) + p*a*np.dot(np.mat(delta_fn(x_0)).T, d):

            # full Armijo condition (in this test, can not reach the optimal point)
            # if fn(x_0 + a * d) <= fn(x_0) + p*a*np.dot(np.mat(delta_fn(x_0)).T, d) and \
            #     fn(x_0 + a * d) >= fn(x_0) + (1-p)*a*np.dot(np.mat(delta_fn(x_0)).T, d):

            # full Wolfe condition (in this test, can reach the optimal point)
            if fn(x_0 + a * d) <= fn(x_0) + p*a*np.dot(np.mat(delta_fn(x_0)).T, d) and \
                np.dot(np.mat(delta_fn(x_0 + a * d)).T, d) >= sigma * np.dot(np.mat(delta_fn(x_0)).T, d):
                break
            m += 1
        
        d_x = b**m * d
        prev_x_0 = x_0
        x_0 += d_x
        d_f = np.matrix(delta_fn(x_0) - delta_fn(prev_x_0))
        if np.dot(d_x.T, d_f) != 0:
            D = D + (1.0 / np.dot(d_x.T, d_f) + np.dot(np.dot(d_f.T, D), d_f) / np.dot(d_x.T, d_f)**2 ) * np.dot(d_x, d_x.T)
            D = D - 1.0 / np.dot(d_x.T, d_f) * ( np.dot(np.dot(D, d_f), d_x.T) + np.dot(np.dot(d_x, d_f.T), D) )
        results.append((x_0, fn(x_0)))
    return results

if __name__ == '__main__':
    """running mode"""
    ''' test function '''
    #function
    def fn(x):
        return 100 * (x[0,0] ** 2 - x[1,0]) ** 2 + (x[0,0] - 1) ** 2
    
    #dleta function
    def delta_fn(x):
        result = np.zeros((2, 1))
        result[0, 0] = 400 * x[0,0] * (x[0,0] ** 2 - x[1,0]) + 2 * (x[0,0] - 1)
        result[1, 0] = -200 * (x[0,0] ** 2 - x[1,0])
        return result
    
    ''' test BFGS '''
    x0 = np.mat([[-1.2], [1]])
    results = BFGS(fn, delta_fn, x_0=x0)
    plot_x = np.arange(0, len(results), 1)
    plot_y = [item[1] for item in results]
    draw_result(plot_x, plot_y)

